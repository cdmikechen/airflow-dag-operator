quarkus.kubernetes-client.trust-certs=true
quarkus.log.min-level=${LOG_MIN_LEVEL:DEBUG}
quarkus.log.category."org.apache.airflow".level=${LOG_LEVEL:DEBUG}

# https://quarkiverse.github.io/quarkiverse-docs/quarkus-operator-sdk/dev/index.html
quarkus.operator-sdk.crd.apply=${CRD_AUTO_CREATE:false}
quarkus.operator-sdk.namespaces=${CRD_NAMESPACES:}

airflow.dag.path=${DAG_PATH:/opt/airflow/dags}
airflow.dag.max-thread=${DAG_MAX_THREAD:2}
airflow.dag.default-user=${DAG_DEFAULT_USER:50000}
airflow.dag.ignore-path=${DAG_IGNORE_PATH:.*(.pyc)$}
airflow.dag.support-pause=${DAG_SUPPORT_PAUSE:false}
airflow.dag.scheduler-resource-name=${AIRFLOW_SCHEDULER_RESOURCE_NAME:}
airflow.dag.scheduler-resource-type=${AIRFLOW_SCHEDULER_RESOURCE_TYPE:deployment}
# Here we use the same variables to facilitate the configuration of airflow environment variables
airflow.dag.scheduler-scan-interval=${AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL:300}

# database connection
quarkus.datasource.jdbc=${DAG_SUPPORT_PAUSE:false}
quarkus.datasource.db-kind = ${AIRFLOW_DATABASE_PROTOCOL:postgresql}
quarkus.datasource.username = ${AIRFLOW_DATABASE_USER:postgres}
quarkus.datasource.password = ${AIRFLOW_DATABASE_PASS:postgres}
quarkus.datasource.jdbc.url = ${AIRFLOW_DATABASE_PASS:jdbc:postgresql://localhost:5432/postgres}

# build
quarkus.container-image.builder=jib