apiVersion: airflow.apache.org/v1
kind: Dag
metadata:
  name: example-bash-operator
  namespace: airflow
spec:
  type: dag_file
  dag_name: example_bash_operator
  content: |
    """Example DAG demonstrating the usage of the BashOperator."""

    from datetime import timedelta

    from airflow import DAG
    from airflow.operators.bash import BashOperator
    from airflow.operators.dummy import DummyOperator
    from airflow.utils.dates import days_ago

    args = {
        'owner': 'airflow',
    }

    with DAG(
        dag_id='example_bash_operator',
        default_args=args,
        schedule_interval='0 0 * * *',
        start_date=days_ago(2),
        dagrun_timeout=timedelta(minutes=60),
        tags=['example', 'example2'],
        params={"example_key": "example_value"},
    ) as dag:

        run_this_last = DummyOperator(
            task_id='run_this_last',
        )

        # [START howto_operator_bash]
        run_this = BashOperator(
            task_id='run_after_loop',
            bash_command='echo 1',
        )
        # [END howto_operator_bash]

        run_this >> run_this_last

        for i in range(3):
            task = BashOperator(
                task_id='runme_' + str(i),
                bash_command='echo "{{ task_instance_key_str }}" && sleep 1',
            )
            task >> run_this

        # [START howto_operator_bash_template]
        also_run_this = BashOperator(
            task_id='also_run_this',
            bash_command='echo "run_id={{ run_id }} | dag_run={{ dag_run }}"',
        )
        # [END howto_operator_bash_template]
        also_run_this >> run_this_last

    # [START howto_operator_bash_skip]
    this_will_skip = BashOperator(
        task_id='this_will_skip',
        bash_command='echo "hello world"; exit 99;',
        dag=dag,
    )
    # [END howto_operator_bash_skip]
    this_will_skip >> run_this_last

    if __name__ == "__main__":
        dag.cli()
---
apiVersion: airflow.apache.org/v1
kind: Dag
metadata:
  name: example-other-file
  namespace: airflow
spec:
  type: file
  path: test_file/
  file_name: test_file.txt
  content: |
    test file content
---
apiVersion: airflow.apache.org/v1
kind: Dag
metadata:
  name: tutorial-yaml
  namespace: airflow
spec:
  type: dag_yaml
  dag_name: tutorial
  dag_yaml:
    title: |
      ### Tutorial Documentation
      Documentation that goes along with the Airflow tutorial located
      [here](https://airflow.apache.org/tutorial.html)
    import_libs:
      - "from datetime import timedelta"
      - "from airflow import DAG"
      - "from airflow.operators.bash import BashOperator"
      - "from airflow.utils.dates import days_ago"
    default_args:
      owner: '''airflow'''
      depends_on_past: false
      email:
        - "airflow@example.com"
      email_on_failure: false
      email_on_retry: false
      retries: 1
      retry_delay: "timedelta(minutes=5)"
      start_date: "days_ago(2)"
    description: "A simple tutorial DAG"
    schedule_interval: "timedelta(days=1)"
    tags:
      - "example"
    python_codes: |
      templated_command = """
      {% for i in range(5) %}
          echo "{{ ds }}"
          echo "{{ macros.ds_add(ds, 7)}}"
          echo "{{ params.my_param }}"
      {% endfor %}
      """
    tasks:
      - task_id: "print_date"
        operator: "BashOperator"
        bash_command: '''date'''
      - task_id: "sleep"
        operator: "BashOperator"
        bash_command: '''sleep 5'''
        depends_on_past: false
        retries: 3
        dependencies:
          - "print_date"
      - task_id: "templated"
        operator: "BashOperator"
        depends_on_past: false
        bash_command: "templated_command"
        params:
          - name: "my_param"
            value: '''Parameter I passed in'''
        dependencies:
          - "print_date"
